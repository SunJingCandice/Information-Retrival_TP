{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file is made to create and use an inverted index.\n",
    "#@author: Thorsten\n",
    "invertedIndex=None #the inverted index, dictionary of lists\n",
    "index=None # the normal index, matrix[doc][word]=count, line-normalized\n",
    "wordList=None #the list of words in the order of the columns of the index = order of rows of inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess text, e. g. the queries as first step of their execution,\n",
    "#but also by the functino buildIndex.\n",
    "#@param text: Strings, the text that should be preprocessed.\n",
    "#@return List of strings: the preprocessed input list, splitted to words\n",
    "#@author: Thorsten, parts taken from Hailian's LDA_Topic_Model.\n",
    "\n",
    "# import stopword and punctuation lists, taken from Hailian\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "nltk.download(\"stopwords\")\n",
    "esw = stopwords.words(\"english\")\n",
    "\n",
    "#prepare removing the punctuation\n",
    "outtab=''\n",
    "for i in punctuation:\n",
    "    outtab+=' '\n",
    "trantab = str.maketrans(punctuation, outtab)\n",
    "\n",
    "ps=nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text=text.lower()\n",
    "    #remove punctuation by replacing with spaces\n",
    "    #If just removed, words might merge.\n",
    "    text=text.translate(trantab)\n",
    "    #remove double spaces\n",
    "    while('  ' in text):\n",
    "        text=text.replace('  ', ' ')\n",
    "    text=text.split(' ')\n",
    "    i=0\n",
    "    while(i<len(text)): #trick because empty strings will be deleted\n",
    "        #remove stopwords\n",
    "        if(text[i] in esw):\n",
    "            del text[i]\n",
    "            continue\n",
    "        text[i]=ps.stem(text[i]).strip()\n",
    "        if(text[i]==''):\n",
    "            del text[i]\n",
    "            continue\n",
    "        i+=1\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the index using the given documents.\n",
    "#@param docs: List of list strings, the content of the documents which should be indexed, after preprocessing,\n",
    "#    so every outer list contains a list of strings which are one document.\n",
    "#@param ids: List, the IDs of the documents in the same order as in docs.\n",
    "#    Used later for identifying the documents indexed.\n",
    "#@author: Thorsten\n",
    "import numpy as np\n",
    "def buildIndex(docs, ids):\n",
    "    global invertedIndex\n",
    "    global index\n",
    "    global wordList\n",
    "    #get all words, but every word just one time\n",
    "    wordList=set()\n",
    "    for document in docs:\n",
    "        for word in document:\n",
    "            wordList.add(word)\n",
    "    wordList=np.sort(np.array(list(wordList)))\n",
    "    #build the index\n",
    "    index=np.zeros([len(docs), len(wordList)])\n",
    "    for i in range(len(docs)):\n",
    "        docs[i]=np.sort(docs[i])\n",
    "        pointer=0\n",
    "        for j in range(len(docs[i])):\n",
    "            while(wordList[pointer]<docs[i][j]):\n",
    "                pointer+=1\n",
    "            index[i][pointer]+=1\n",
    "    #build the inverted index\n",
    "    invertedIndex={}\n",
    "    for word in wordList:\n",
    "        invertedIndex[word]=[]\n",
    "    for i in range(len(docs)):\n",
    "        for k in invertedIndex.keys():\n",
    "            if k in docs[i]:\n",
    "                invertedIndex[k].append(ids[i])\n",
    "    #Till now the index only contains TF (term frequency).\n",
    "    #Now let's add IDF (inverdet document frequency)\n",
    "    for j in range(len(wordList)):\n",
    "        index[:,j]/=len(invertedIndex[wordList[j]])\n",
    "    #normalize\n",
    "    for i in range(len(docs)):\n",
    "        index[i]/=np.linalg.norm(index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing\n",
    "#import time\n",
    "#teststring='This is a test-string!            Just play/trick. asdf he is, she was , dump, dumps, trains, simulations.'\n",
    "#docs=\"This is just a simple test document. Try and let's see what happens. Just test, test and test.\"\n",
    "#dump='test test test test test'\n",
    "#teststring=preprocess(teststring)\n",
    "#docs=preprocess(docs)\n",
    "#dump=preprocess(dump)\n",
    "#timeBefore=time.perf_counter()\n",
    "#buildIndex([teststring, docs, dump], [0, 1, 2])\n",
    "#timeAfter=time.perf_counter()\n",
    "#print(timeAfter-timeBefore)\n",
    "#print(invertedIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hardcore test\n",
    "#import loadDataRaw\n",
    "#import time\n",
    "#documents=np.array(loadDataRaw.readFile('train.docs'))\n",
    "#ids=documents[:,0]\n",
    "#documents=documents[:,1]\n",
    "#timeBefore=time.perf_counter()\n",
    "#documents=[preprocess(doc) for doc in documents]\n",
    "#timeAfter=time.perf_counter()\n",
    "#print('Time for preprocessing: ', timeAfter-timeBefore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeBefore=time.perf_counter()\n",
    "#buildIndex(documents, ids)\n",
    "#timeAfter=time.perf_counter()\n",
    "#print('Time for index building: ', timeAfter-timeBefore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

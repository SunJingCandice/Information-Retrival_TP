{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file measures the performance of the query execution, using the other files and their functionalities as imports.\n",
    "#Based on PerformanceMeasure, same output format but specialized on the inverted index files.\n",
    "#@author Thorsten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and initialize the query execution\n",
    "import executeQuery_InvertedIndex as eq\n",
    "import loadDataRaw\n",
    "import numpy as np\n",
    "import time #time measurement for the performance measure\n",
    "import gc #gc.collect() runs garbage collection manually\n",
    "import csv\n",
    "documents=np.array(loadDataRaw.readFile('train.docs'))\n",
    "#here don't give the document names as strings to the index becuasae the returned results would be so big\n",
    "#that only a few hundret fit into memory\n",
    "#Instead give the position in the index and store in docIDs to be able to lookup there for fast reconverting later.\n",
    "docIDs=documents[:,0]\n",
    "#eq.initExecution(documents[:,1], documents[:,0])\n",
    "eq.initExecution(documents[:,1], np.array(range(len(documents))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing, not needed later\n",
    "#query=('This is a test!')\n",
    "#query=('how contaminated are our children ?')\n",
    "#next query: Original from query document, so indexed\n",
    "#query='statin breast cancer survival nationwide cohort study finland abstract recent studies suggested statins established drug group prevention cardiovascular mortality delay prevent breast cancer recurrence effect disease-specific mortality remains unclear evaluated risk breast cancer death statin users population-based cohort breast cancer patients study cohort included newly diagnosed breast cancer patients finland num num num cases identified finnish cancer registry information statin diagnosis obtained national prescription database cox proportional hazards regression method estimate mortality statin users statin time-dependent variable total num participants statins median follow-up num years diagnosis range num num years num participants died num num due breast cancer adjustment age tumor characteristics treatment selection post-diagnostic pre-diagnostic statin lowered risk breast cancer death hr num num ci num num hr num num ci num num risk decrease post-diagnostic statin affected healthy adherer bias greater likelihood dying cancer patients discontinue statin association dose-dependent observed low-dose/short-term dose time-dependence survival benefit pre-diagnostic statin users suggests causal effect evaluated clinical trial testing statins effect survival breast cancer patients '\n",
    "#timeBefore=time.perf_counter()\n",
    "#result=eq.executeQuery(query, sort=False)\n",
    "#timeAfter=time.perf_counter()\n",
    "#print(timeAfter-timeBefore)\n",
    "#print('min: ', np.min(result[1]))\n",
    "#print('max: ', np.max(result[1]))\n",
    "#print('avg: ', np.average(result[1]))\n",
    "#print('len: ', len(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query file\n",
    "#choose one of them\n",
    "#queries=np.array(loadDataRaw.readFile('train.nontopic-titles.queries'))\n",
    "#queries=np.array(loadDataRaw.readFile('train.vid-desc.queries'))\n",
    "#queries=np.array(loadDataRaw.readFile('train.vid-titles.queries'))\n",
    "#queries=np.array(loadDataRaw.readFile('train.all.queries'))\n",
    "queries=np.array(loadDataRaw.readFile('train.titles.queries'))\n",
    "\n",
    "#queries=np.array(loadDataRaw.readFile('train.titles.queries')[0:10]) #subset for testing\n",
    "ids=queries[:,0] #now containing all ids\n",
    "queries=queries[:,1] #now containing all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute the queries\n",
    "runtimes=np.empty(len(queries))\n",
    "resultIDs=np.empty(len(queries), dtype=object)\n",
    "resultSims=np.empty(len(queries), dtype=object)\n",
    "print('Execute ', len(queries), ' queries...')\n",
    "for i in range(len(queries)):\n",
    "    timeBefore=time.perf_counter()\n",
    "    result=eq.executeQuery(queries[i], sort=False)\n",
    "    timeAfter=time.perf_counter()\n",
    "    runtimes[i]=timeAfter-timeBefore\n",
    "    resultIDs[i]=result[0]\n",
    "    resultSims[i]=result[1]\n",
    "    gc.collect() #run the garbage collection to avoid running out of memory\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postprocessing\n",
    "#some statistics\n",
    "print('Complete runtime in s:\\t', sum(runtimes))\n",
    "print('Min runtime in ms:\\t', min(runtimes)*1000)\n",
    "print('Max runtime in ms:\\t', max(runtimes)*1000)\n",
    "print('Avg runtime in ms:\\t', np.average(runtimes)*1000)\n",
    "print('Variance oÂ² in ms:\\t', np.var(runtimes)*1000)\n",
    "print('Std o in ms:\\t\\t', np.std(runtimes)*1000)\n",
    "#tidy up to avoid running out of memory\n",
    "#del runtimes\n",
    "#del generateDTM\n",
    "#gc.collect() #manual run of garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the short versoin without the gold standard values\n",
    "print(\"Start writing the short result version to disk.\")\n",
    "with open('Results_short.csv', 'w', encoding='utf-8') as csvOutput: #change file name as needed\n",
    "    writer=csv.writer(csvOutput, lineterminator='\\n', delimiter='\\t')\n",
    "    #headers\n",
    "    writer.writerow(['QUERY_ID', 'DOC_ID', 'sim_results'])\n",
    "    for i in range(len(ids)): #for every query id...\n",
    "        for j in range(len(resultIDs[i])):\n",
    "            writer.writerow([ids[i], resultIDs[i][j], resultSims[i][j]])\n",
    "print(\"Completed writing the short result version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the gold standard\n",
    "goldStandard=np.array(loadDataRaw.readFile('train.3-2-1.qrel'))\n",
    "goldStandard=np.delete(goldStandard, 1, 1) #now [queryID][docID][relevance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the comparing\n",
    "#Build the result matix\n",
    "mat=np.zeros((4, 4)) #Format: sum, count, min, max\n",
    "mat[:,2]=1\n",
    "mat[:,3]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join queries and gold standard and fill the matrix\n",
    "#sortingn becomes a little tricky\n",
    "gc.collect()\n",
    "idOrder=np.argsort(ids)\n",
    "ids=ids[idOrder]\n",
    "resultIDs=resultIDs[idOrder]\n",
    "resultSims=resultSims[idOrder]\n",
    "runtimes=runtimes[idOrder]\n",
    "del idOrder\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    order=np.argsort(resultIDs[i])\n",
    "    resultIDs[i]=resultIDs[i][order]\n",
    "    resultSims[i]=resultSims[i][order]\n",
    "del order\n",
    "\n",
    "goldStandard=goldStandard[goldStandard[:,1].argsort()]\n",
    "goldStandard=goldStandard[goldStandard[:,0].argsort(kind='mergesort')]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepere something to write later\n",
    "file=[] #stores what to write later into the csv-file\n",
    "\n",
    "#headers\n",
    "file.append([])\n",
    "for name in ['queryID', 'queryRuntime', 'docResultID', 'resultSimilarity', 'goldStandardValue']:\n",
    "    file[0]+=[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now merge the arrays and fill the file variable\n",
    "#Remember you must use the lookup table docIDs for the resultIDs.\n",
    "gsPointer=0 #pointer to the current field in the gold standard\n",
    "notFound=0\n",
    "#    because of the problem that one result id can have many entries in the gold standard,\n",
    "#    but not the other way around.\n",
    "for i in range(len(ids)): #for every query id...\n",
    "    while((gsPointer<len(goldStandard)) and (ids[i]>goldStandard[gsPointer][0])): #missed a gold standard entry, so sth went wrong\n",
    "        #Critical error only if all queries are made, but decided to keep on working\n",
    "        #print('Error: i=', i, ' gsPointer=', gsPointer, ' Missed Gold standard entry=', goldStandard[gsPointer], 'id=', ids[i])\n",
    "        gsPointer+=1\n",
    "        notFound+=1\n",
    "    for j in range(len(resultIDs[i])):\n",
    "        if((gsPointer<len(goldStandard)) and ((ids[i]==goldStandard[gsPointer][0]) & (docIDs[resultIDs[i][j]]==goldStandard[gsPointer][1]))): #match\n",
    "            mat[int(goldStandard[gsPointer][2])][0]+=resultSims[i][j]\n",
    "            mat[int(goldStandard[gsPointer][2])][1]+=1\n",
    "            mat[int(goldStandard[gsPointer][2])][2]=min(mat[int(goldStandard[gsPointer][2])][2], resultSims[i][j])\n",
    "            mat[int(goldStandard[gsPointer][2])][3]=max(mat[int(goldStandard[gsPointer][2])][3], resultSims[i][j])\n",
    "            file.append([ids[i], runtimes[i], docIDs[resultIDs[i][j]], resultSims[i][j], goldStandard[gsPointer][2]])\n",
    "            gsPointer+=1\n",
    "        else: #search result not in the gold standard, meaning it has importance 0\n",
    "            mat[0][0]+=resultSims[i][j]\n",
    "            mat[0][1]+=1\n",
    "            mat[0][2]=min(mat[0][2], resultSims[i][j])\n",
    "            mat[0][3]=max(mat[0][3], resultSims[i][j])\n",
    "            file.append([ids[i], runtimes[i], docIDs[resultIDs[i][j]], resultSims[i][j], '0'])\n",
    "while(gsPointer<len(goldStandard)):\n",
    "    notFound+=1\n",
    "    gsPointer+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print statistics for the results\n",
    "print('Not found from the gold standard (errors): ', notFound)\n",
    "print('Result performance value matrix:')\n",
    "print('\\t\\tsum, count, min, max')\n",
    "print('importance 0:\\t', mat[0], 'Avg: ', mat[0][0]/mat[0][1])\n",
    "print('importance 1:\\t', mat[1], 'Avg: ', mat[1][0]/mat[1][1])\n",
    "print('importance 2:\\t', mat[2], 'Avg: ', mat[2][0]/mat[2][1])\n",
    "print('importance 3:\\t', mat[3], 'Avg: ', mat[3][0]/mat[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wirte the results to disk\n",
    "#del runtimes, ids, resultIDs, resultSims, goldStandard\n",
    "\n",
    "#export\n",
    "print(\"Start writing the results to disk.\")\n",
    "with open('Results.csv', 'w', encoding='utf-8') as csvOutput: #change file name as needed\n",
    "    writer=csv.writer(csvOutput, lineterminator='\\n', delimiter='\\t')\n",
    "    for i in range(len(file)):\n",
    "        writer.writerow(file[i])\n",
    "print(\"Completed writing the results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

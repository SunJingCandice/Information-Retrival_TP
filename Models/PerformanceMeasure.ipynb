{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file measures the performance of the query execution, using the other files and their functionalities as imports.\n",
    "#@author Thorsten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the index file which generates the index\n",
    "import DTM_generate\n",
    "#import IRByLDATopicModel as DTM_generate #trick becuase the filename changed for newer versions\n",
    "#import IRByRandomLeaderPreClustering_final as DTM_generate\n",
    "#import IRByRandomLeaderPreClustering_stemmed as DTM_generate\n",
    "#import IRByTieredIndex as DTM_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and initialize the query execution\n",
    "import executeQuery as eq\n",
    "import loadDataRaw\n",
    "import numpy as np\n",
    "import time #time measurement for the performance measure\n",
    "import gc #gc.collect() runs garbage collection manually\n",
    "eq.initExecution(DTM_generate.train_tfidf, DTM_generate.train_doc_file['id'], DTM_generate.get_QueryVector)\n",
    "#eq.initExecution(DTM_generate.train_tfidf, DTM_generate.train_doc_file['id'], DTM_generate.get_QueryVector_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing, not needed later\n",
    "query=(['This is a test!'])\n",
    "#query=(['how contaminated are our children ?'])\n",
    "timeBefore=time.perf_counter()\n",
    "result=eq.executeQuery(query, sort=False)\n",
    "timeAfter=time.perf_counter()\n",
    "print(timeAfter-timeBefore)\n",
    "print(result)\n",
    "print('min: ', np.min(result[1]))\n",
    "print('max: ', np.max(result[1]))\n",
    "print('avg: ', np.average(result[1]))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query file\n",
    "#choose one of them\n",
    "#queries=np.array(loadDataRaw.readFile('train.nontopic-titles.queries'))\n",
    "#queries=np.array(loadDataRaw.readFile('train.vid-desc.queries'))\n",
    "#queries=np.array(loadDataRaw.readFile('train.vid-titles.queries'))\n",
    "#queries=np.array(loadDataRaw.readFile('train.all.queries'))\n",
    "queries=np.array(loadDataRaw.readFile('train.titles.queries'))\n",
    "\n",
    "#queries=np.array(loadDataRaw.readFile('train.titles.queries')[0:10]) #subset for testing\n",
    "ids=queries[:,0] #now containing all ids\n",
    "queries=queries[:,1] #now containing all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute the queries\n",
    "runtimes=np.zeros(len(queries))\n",
    "resultIDs=[]\n",
    "resultSims=[]\n",
    "print('Execute ', len(queries), ' queries...')\n",
    "for i in range(len(queries)):\n",
    "    timeBefore=time.perf_counter()\n",
    "    result=eq.executeQuery([queries[i]], threshold=-1, sort=False)\n",
    "    timeAfter=time.perf_counter()\n",
    "    runtimes[i]=timeAfter-timeBefore\n",
    "    resultIDs.append(result[0])\n",
    "    resultSims.append(result[1])\n",
    "print('Done!')\n",
    "resultIDs=np.array(resultIDs)\n",
    "resultSims=np.array(resultSims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postprocessing\n",
    "#some statistics\n",
    "print('Complete runtime in s:\\t', sum(runtimes))\n",
    "print('Min runtime in ms:\\t', min(runtimes)*1000)\n",
    "print('Max runtime in ms:\\t', max(runtimes)*1000)\n",
    "print('Avg runtime in ms:\\t', np.average(runtimes)*1000)\n",
    "print('Variance oÂ² in ms:\\t', np.var(runtimes)*1000)\n",
    "print('Std o in ms:\\t\\t', np.std(runtimes)*1000)\n",
    "#tidy up to avoid running out of memory\n",
    "#del runtimes\n",
    "#del generateDTM\n",
    "#gc.collect() #manual run of garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the gold standard\n",
    "goldStandard=np.array(loadDataRaw.readFile('train.3-2-1.qrel'))\n",
    "goldStandard=np.delete(goldStandard, 1, 1) #now [queryID][docID][relevance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the comparing\n",
    "#Build the result matix\n",
    "mat=np.zeros((4, 4)) #Format: sum, count, min, max\n",
    "mat[:,2]=1\n",
    "mat[:,3]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join queries and gold standard and fill the matrix\n",
    "#sortingn becomes a little tricky\n",
    "gc.collect()\n",
    "idOrder=np.argsort(ids)\n",
    "ids=ids[idOrder]\n",
    "resultIDs=resultIDs[idOrder]\n",
    "resultSims=resultSims[idOrder]\n",
    "runtimes=runtimes[idOrder]\n",
    "del idOrder\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    order=np.argsort(resultIDs[i])\n",
    "    resultIDs[i]=resultIDs[i][order]\n",
    "    resultSims[i]=resultSims[i][order]\n",
    "del order\n",
    "\n",
    "goldStandard=goldStandard[goldStandard[:,1].argsort()]\n",
    "goldStandard=goldStandard[goldStandard[:,0].argsort(kind='mergesort')]\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepere something to write later\n",
    "file=[] #stores what to write later into the csv-file\n",
    "\n",
    "#headers\n",
    "file.append([])\n",
    "for name in ['queryID', 'queryRuntime', 'docResultID', 'resultSimilarity', 'goldStandardValue']:\n",
    "    file[0]+=[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now merge the arrays and fill the file variable\n",
    "gsPointer=0 #pointer to the current field in the gold standard\n",
    "notFound=0\n",
    "#    because of the problem that one result id can have many entries in the gold standard,\n",
    "#    but not the other way around.\n",
    "for i in range(len(ids)): #for every query id...\n",
    "    while((gsPointer<len(goldStandard)) and (ids[i]>goldStandard[gsPointer][0])): #missed a gold standard entry, so sth went wrong\n",
    "        #Critical error only if all queries are made, but decided to keep on working\n",
    "        #print('Error: i=', i, ' gsPointer=', gsPointer, ' Missed Gold standard entry=', goldStandard[gsPointer], 'id=', ids[i])\n",
    "        gsPointer+=1\n",
    "        notFound+=1\n",
    "    for j in range(len(resultIDs[i])):\n",
    "        if((gsPointer<len(goldStandard)) and ((ids[i]==goldStandard[gsPointer][0]) & (resultIDs[i][j]==goldStandard[gsPointer][1]))): #match\n",
    "            mat[int(goldStandard[gsPointer][2])][0]+=resultSims[i][j]\n",
    "            mat[int(goldStandard[gsPointer][2])][1]+=1\n",
    "            mat[int(goldStandard[gsPointer][2])][2]=min(mat[int(goldStandard[gsPointer][2])][2], resultSims[i][j])\n",
    "            mat[int(goldStandard[gsPointer][2])][3]=max(mat[int(goldStandard[gsPointer][2])][3], resultSims[i][j])\n",
    "            file.append([ids[i], runtimes[i], resultIDs[i][j], resultSims[i][j], goldStandard[gsPointer][2]])\n",
    "            gsPointer+=1\n",
    "        else: #search result not in the gold standard, meaning it has importance 0\n",
    "            mat[0][0]+=resultSims[i][j]\n",
    "            mat[0][1]+=1\n",
    "            mat[0][2]=min(mat[0][2], resultSims[i][j])\n",
    "            mat[0][3]=max(mat[0][3], resultSims[i][j])\n",
    "            file.append([ids[i], runtimes[i], resultIDs[i][j], resultSims[i][j], '0'])\n",
    "while(gsPointer<len(goldStandard)):\n",
    "    notFound+=1\n",
    "    gsPointer+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print statistics for the results\n",
    "print('Not found from the gold standard (errors): ', notFound)\n",
    "print('Result performance value matrix:')\n",
    "print('\\t\\tsum, count, min, max')\n",
    "print('importance 0:\\t', mat[0], 'Avg: ', mat[0][0]/mat[0][1])\n",
    "print('importance 1:\\t', mat[1], 'Avg: ', mat[1][0]/mat[1][1])\n",
    "print('importance 2:\\t', mat[2], 'Avg: ', mat[2][0]/mat[2][1])\n",
    "print('importance 3:\\t', mat[3], 'Avg: ', mat[3][0]/mat[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wirte the results to disk\n",
    "#del runtimes, ids, resultIDs, resultSims, goldStandard\n",
    "\n",
    "#export\n",
    "import csv\n",
    "print(\"Start writing the results to disk.\")\n",
    "with open('Results.csv', 'w', encoding='utf-8') as csvOutput: #change file name as needed\n",
    "    writer=csv.writer(csvOutput, lineterminator='\\n', delimiter='\\t')\n",
    "    for i in range(len(file)):\n",
    "        writer.writerow(file[i])\n",
    "print(\"Completed writing the results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

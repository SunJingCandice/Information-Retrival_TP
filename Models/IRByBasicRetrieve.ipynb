{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "esw = stopwords.words(\"english\")\n",
    "from string import punctuation\n",
    "\n",
    "#remove \"num\", because \"num\" has the highest term frequency(45538) at the orinal file, \n",
    "#the second most frequent term only has a freq. of 3750\n",
    "#esw = esw + ['abstract', 'ci', 'hr','l','pubmed', 'num'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import datetime\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####read file\n",
    "\n",
    "#read train.docs file\n",
    "train_doc_file = pd.read_csv(\"train.docs\", encoding = 'utf-8', sep='\\t', header=None)\n",
    "train_doc_file.columns = ['id', 'text']\n",
    "\n",
    "#read train.nontopic-titles.queries file\n",
    "train_query_file = pd.read_csv(\"train.nontopic-titles.queries\", encoding = 'utf-8', sep='\\t', header=None)\n",
    "train_query_file.columns = ['id', 'text']\n",
    "\n",
    "#read example query file, only one query\n",
    "#train_query_file = pd.read_csv(\"example.queries\", encoding = 'utf-8', sep='\\t', header=None)\n",
    "#train_query_file.columns = ['id', 'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: build functions for basic retrieve\n",
    "text is not preprocessed\n",
    "\n",
    "1. build DTM, query vector\n",
    "2. IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### build functions to generate document-term matrix\n",
    "\n",
    "## get DTM, weighted by tfidf, the sqrt of the query vector is 1\n",
    "# therefore, in the retrive phase, \n",
    "# dot product of the doc vector and query vector can be used to represent the cosine similarity\n",
    "\n",
    "# input format: train_doc_file.text\n",
    "\n",
    "def get_DTM_tfidf(file):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(file)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    #print(vocabulary)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    \n",
    "    X_train_tfidf = CountVectorizer(vocabulary = vocabulary)\n",
    "    X_train_tfidf = X_train_tfidf.fit_transform(file)\n",
    "    \n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_tfidf)\n",
    "    return X_train_tfidf\n",
    "\n",
    "\n",
    "## get DTM, weighted by term frequency\n",
    "def get_DTM_tf(file):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(file)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "\n",
    "    X_train_tf = CountVectorizer(vocabulary = vocabulary)\n",
    "    X_train_tf = X_train_tf.fit_transform(file)\n",
    "    return X_train_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Generate query vector for each query\n",
    "\n",
    "# get_QueryVector_tfidf helps to get the tiidf weighted query vector, the sqrt of the query vector is 1\n",
    "# therefore, in the retrive phase, \n",
    "# dot product of the doc vector and query vector can be used to represent the cosine similarity\n",
    "\n",
    "# input format: train_doc_file.text, train_query_file.text\n",
    "\n",
    "\n",
    "def get_QueryVector_tfidf(queryFile, docFile):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(docFile)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    \n",
    "    if type(queryFile) is str:\n",
    "        query_vect = []\n",
    "        query = queryFile\n",
    "        query = query.split()\n",
    "        frequency = defaultdict(int)\n",
    "        for el in vocabulary:\n",
    "            if el in query:\n",
    "                frequency[el]+= 1\n",
    "            else:\n",
    "                frequency[el] = 0\n",
    "        query_vect = list(dict(frequency).values())/np.linalg.norm(list(dict(frequency).values()))\n",
    "        \n",
    "    else:\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        \n",
    "        query_vect = CountVectorizer(analyzer = \"word\", vocabulary = vocabulary)\n",
    "        query_vect = query_vect.fit_transform(queryFile)\n",
    "        \n",
    "        query_vect = tfidf_transformer.fit_transform(query_vect)\n",
    "    return query_vect\n",
    "\n",
    "\n",
    "\n",
    "###Generate the query vector, weighted by term frequency\n",
    "def get_QueryVector(queryFile, docFile):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(docFile)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    \n",
    "    if type(queryFile) is str:\n",
    "        query_vect = []\n",
    "        query = queryFile\n",
    "        query = query.split()\n",
    "        frequency = defaultdict(int)\n",
    "        for el in vocabulary:\n",
    "            if el in query:\n",
    "                frequency[el]+= 1\n",
    "            else:\n",
    "                frequency[el] = 0\n",
    "        query_vect = list(dict(frequency).values())\n",
    "        \n",
    "    else:\n",
    "        query_vect = CountVectorizer(analyzer = \"word\", vocabulary = vocabulary)\n",
    "        query_vect = query_vect.fit_transform(queryFile)\n",
    "        \n",
    "    return query_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create tfidf weighted DTM for the train.docs file\n",
    "train_tfidf = get_DTM_tfidf(train_doc_file.text)\n",
    "train_tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### create query vector matrix for the train.nontopic-titles.queries file \n",
    "query_vect = get_QueryVector_tfidf(train_query_file.text, train_doc_file.text)\n",
    "query_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.dot(query_vect, train_tfidf.transpose())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### information retrieve \n",
    "\n",
    "x = np.dot(query_vect, train_tfidf.transpose())\n",
    "documents_id = list(train_doc_file.id)\n",
    "query_id = list(train_query_file.id)\n",
    "r = []\n",
    "for i in range(x.shape[0]):\n",
    "    sims = []\n",
    "    IR_doc_sims = []\n",
    "    IR_doc = []\n",
    "    \n",
    "    for j in range(x.shape[1]): \n",
    "        sims.append(x[i][0,j])\n",
    "    \n",
    "    for el in sims:\n",
    "        if el>0:\n",
    "            IR_doc_sims.append(el)\n",
    "            IR_doc.append(documents_id[sims.index(el)])\n",
    "\n",
    "    if len(IR_doc_sims) >0 and len(IR_doc)>0:\n",
    "        IR_doc_sims, IR_doc= zip(*sorted(zip(IR_doc_sims, IR_doc), reverse=True))# rank the results\n",
    "    for m in range(len(IR_doc)):\n",
    "        #if IR_doc_sims[j]> sum(IR_doc_sims)/len(IR_doc_sims):\n",
    "        r.append([str(query_id[i]) ,  str(IR_doc[m]), IR_doc_sims[m]])        \n",
    "        \n",
    "df = pd.DataFrame(r, columns = ['QUERY_ID', 'DOC_ID', 'sim_results'])\n",
    "df.to_csv('IRByBasicRetrieve.txt', header=None, index=None, sep=' ', mode='a')       \n",
    "df       \n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import matplotlib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "esw = stopwords.words(\"english\")\n",
    "from string import punctuation\n",
    "\n",
    "#remove \"num\", because \"num\" has the highest term frequency(45538) at the orinal file, \n",
    "#the second most frequent term only has a freq. of 3750\n",
    "esw = esw + ['abstract', 'ci', 'hr','l','pubmed', 'num'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####read file\n",
    "\n",
    "#read train.docs file\n",
    "train_doc_file = pd.read_csv(\"train.docs\", encoding = 'utf-8', sep='\\t', header=None)\n",
    "train_doc_file.columns = ['id', 'text']\n",
    "\n",
    "#read train.nontopic-titles.queries file\n",
    "train_query_file = pd.read_csv(\"train.nontopic-titles.queries\", encoding = 'utf-8', sep='\\t', header=None)\n",
    "train_query_file.columns = ['id', 'text']\n",
    "\n",
    "\n",
    "\n",
    "### preprocessing\n",
    "\n",
    "train_doc_file[\"text\"] = train_doc_file['text'].str.replace('/', ' or ')\n",
    "train_query_file[\"text\"] = train_doc_file['text'].str.replace('/', ' or ')\n",
    "\n",
    "def removeStopwords(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([word for word in el.lower().split() if word not in esw])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "def removePunctuation(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([word for word in el.lower().split() if word not in punctuation])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove stopwords\n",
    "removeStopwords(train_doc_file['text'])\n",
    "removeStopwords(train_query_file['text'])\n",
    "\n",
    "### remove punctuation\n",
    "removePunctuation(train_doc_file['text'])\n",
    "removePunctuation(train_query_file['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create DTM\n",
    "\n",
    " ## get DTM, weighted by tfidf\n",
    "def get_DTM_tfidf(file):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(file)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    return X_train_tfidf\n",
    "\n",
    "\n",
    "## get DTM, weighted by tfidf\n",
    "def get_DTM_tf(file):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(file)\n",
    "\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "    X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "    return X_train_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create tfidf weighted DTM for the train.docs file\n",
    "train_tfidf = get_DTM_tfidf(train_doc_file.text)\n",
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Generate the query vector\n",
    "def get_QueryVector(queryFile, docFile=train_doc_file.text):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(docFile)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    query_vect = CountVectorizer(analyzer = \"word\", vocabulary = vocabulary)\n",
    "    query_vect = query_vect.fit_transform(queryFile)\n",
    "    return query_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create query vector matrix for the train.nontopic-titles.queries file \n",
    "query_vect = get_QueryVector(train_query_file.text, train_doc_file.text)\n",
    "query_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preclustering through randomly selected leaders\n",
    "\n",
    "index = []\n",
    "topic_clustering = []\n",
    "for i in range(int(math.sqrt(train_tfidf.shape[0]))):\n",
    "    index.append(random.randint(0,train_tfidf.shape[0])) # randomly select doc leaders index\n",
    "    topic_clustering.append([]) # initiate topic clustering\n",
    "\n",
    "for i in range(train_tfidf.shape[0]):\n",
    "    sims = []\n",
    "    #if i not in index:\n",
    "    for el in index:\n",
    "        sims.append(np.dot(train_tfidf[i,], train_tfidf[el,].transpose())[0,0])\n",
    "        #print(sims)    \n",
    "        #maxSim = max(sims)\n",
    "    maxsimindex = sims.index(max(sims))\n",
    "    topic_clustering[maxsimindex].append(i)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### information retrieve \n",
    "def preclusteringByRandomLeader(docFile, leaderNumber):\n",
    "    \n",
    "    train_tfidf = docFile\n",
    "    leaderIndex = []\n",
    "    topic_clustering = []\n",
    "    \n",
    "    for i in range(leaderNumber):\n",
    "        leaderIndex.append(random.randint(0,train_tfidf.shape[0])) # randomly select doc leaders index\n",
    "        topic_clustering.append([]) # initiate topic clustering\n",
    "\n",
    "    for i in range(train_tfidf.shape[0]):\n",
    "        sims = []\n",
    "        #if i not in index:\n",
    "        for el in leaderIndex:\n",
    "            sims.append(np.dot(train_tfidf[i,], train_tfidf[el,].transpose())[0,0])\n",
    "            #print(sims)    \n",
    "            #maxSim = max(sims)\n",
    "        maxsimindex = sims.index(max(sims))\n",
    "        topic_clustering[maxsimindex].append(i) \n",
    "    \n",
    "    return leaderIndex,topic_clustering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IRqueryByLeaders(leaderIndex, topic_clustering, queryVector ):    \n",
    "    ### get the similarity of query with each doc leader\n",
    "    index = leaderIndex\n",
    "    topic_clustering = topic_clustering\n",
    "    query_vector = queryVector\n",
    "    \n",
    "    for q in range(queryVector.shape[0]):\n",
    "        \n",
    "        sims_leaders = []\n",
    "        for el in index:\n",
    "            sims_leaders.append(np.dot(query_vector[q], train_tfidf[el,].transpose())[0,0])\n",
    "\n",
    "       \n",
    "        maxsimindex = sims_leaders.index(max(sims_leaders)) # get the most similarity clustering index\n",
    "        #print(topic_clustering[maxsimindex])\n",
    "\n",
    "        sims_docs = []\n",
    "        #if len(topic_clustering[maxsimindex])> 3:\n",
    "            #get_sims_docs = []\n",
    "        for el in topic_clustering[maxsimindex]:\n",
    "            sims_docs.append(np.dot(query_vector[q], train_tfidf[el,].transpose())[0,0]) \n",
    "            # get the similarty of query&docs in the most similarity clustering index\n",
    "        \n",
    "        IR_doc_sims = []\n",
    "        IR_doc = []\n",
    "        for i in range(len(topic_clustering[maxsimindex])):\n",
    "            if sims_docs[i]>0:\n",
    "                IR_doc_sims.append(sims_docs[i]) #get the non-zero similarity\n",
    "                IR_doc.append(topic_clustering[maxsimindex][i]) #get the index of the docs with non-zero similarity  \n",
    "\n",
    "\n",
    "        #IR_doc= []\n",
    "        #for el in d:\n",
    "            #IR_doc.append(topic_clustering[maxsimindex][el])\n",
    "\n",
    "\n",
    "        #print(sims_leaders, sims_docs, d, IR_doc, IR_doc_sims)\n",
    "\n",
    "        #print(\"Doc\", \"Similarity\")\n",
    "        for j in range(len(IR_doc)):\n",
    "            print(\"PLAIN-\"+ str(q), \"0\", \"MED-\" + str(IR_doc[j]), IR_doc_sims[j])\n",
    "\n",
    "    #t2 = datetime.datetime.now().time()\n",
    "    \n",
    "    #print(\"time:\",  t1, t2 )\n",
    "    #return sims_docs\n",
    "    \n",
    "    #sims_docs_normalized = sims_docs/total\n",
    "           \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the leader docs index and the docs clusterings with clustering number =  int(math.sqrt(train_tfidf.shape[0]))\n",
    "leaderIndex,topic_clustering =  preclusteringByRandomLeader( train_tfidf, int(math.sqrt(train_tfidf.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the leader docs index and the docs clusterings with clustering number =  int(math.sqrt(train_tfidf.shape[0]))\n",
    "leaderIndex_10,topic_clustering_10 =  preclusteringByRandomLeader( train_tfidf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test the IR on the whole query file\n",
    "t1 =  datetime.datetime.now()\n",
    "IR_results = IRqueryByLeaders(leaderIndex, topic_clustering, query_vect )\n",
    "t2 =  datetime.datetime.now()\n",
    "t = t2-t1\n",
    "print(IR_results)\n",
    "print(\"running time:\", t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the IR on the whole query file\n",
    "t1 =  datetime.datetime.now()\n",
    "IR_results = IRqueryByLeaders(leaderIndex_10, topic_clustering_10, query_vect )\n",
    "t2 =  datetime.datetime.now()\n",
    "t = t2-t1\n",
    "print(IR_results)\n",
    "print(\"running time:\", t )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
